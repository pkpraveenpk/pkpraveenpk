{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32a0819b",
   "metadata": {},
   "source": [
    "## Importing neccessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65b49649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d83bbf8",
   "metadata": {},
   "source": [
    "## Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81210f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of stop words in library :  326 \n",
      "\n",
      "The first 15 stop words \n",
      "\n",
      "['afterwards', 'too', 'should', 'various', 'name', 'herself', 'back', 'already', 'wherever', 'few', 'hereupon', 'go', 'mostly', 'everywhere', 'as']\n"
     ]
    }
   ],
   "source": [
    "en = spacy.load(\"en_core_web_sm\")\n",
    "print('The total number of stop words in library : ',len(en.Defaults.stop_words),'\\n')\n",
    "print('The first 15 stop words \\n')\n",
    "print(list(en.Defaults.stop_words)[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cd47dc",
   "metadata": {},
   "source": [
    "## Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5bf716c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n",
      "cries this lied computing organizing matches \n",
      "\n",
      "Stemming\n",
      "cri this lie comput organ match \n",
      "\n",
      "Lemmatization\n",
      "cry this lie computing organize match "
     ]
    }
   ],
   "source": [
    "tokens=\"cries this lied computing organizing matches\"\n",
    "\n",
    "print('Original')\n",
    "print(tokens,'\\n')\n",
    "\n",
    "print(\"Stemming\")\n",
    "for token in tokens.split(\" \"):\n",
    "    print(stemmer.stem(token),end=\" \")\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"Lemmatization\")\n",
    "lemmatized=en(tokens)\n",
    "\n",
    "for item in lemmatized:\n",
    "    print(item.lemma_,end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6415c159",
   "metadata": {},
   "source": [
    "## Task3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39cc1417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Note from poster to Kubrick newsgroup:\\n\\nI found this on a bbs a while ago and I thought I\\'d pass it along to all \\nof you Kubrick freaks out there.\\n\\n02/23/89\\nTranscriber\\'s note:\\n\\nFor all you Clarke/Kubrick/2001 fans,\\n\\nI found the original paper copy of this screenplay a while back and felt \\ncompelled to transcribe it to disk and upload it to various bulletin \\nboards for the enjoyment of all.\\n\\nThe final movie deviates from this screenplay in a number of interesting \\nways. I\\'ve tried to maintain the format of the original document except \\nthe number of lines per page of the original. In order to reduce the \\nlength of this file I\\'ve used a bar of \"------\" to delimit the pages as \\nthere was a lot of whitespace per original screenplay page.\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt1=open(r\"K:\\Desktop\\NIIT\\Practice\\Course12\\DS3_C2_S3_ScifiscriptsIntro_Data_Practice.txt\").read()\n",
    "\n",
    "txt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55b04687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words : \n",
      "\n",
      "Note poster Kubrick newsgroup : \n",
      "\n",
      " found bbs ago thought pass \n",
      " Kubrick freaks . \n",
      "\n",
      " 02/23/89 \n",
      " Transcriber note : \n",
      "\n",
      " Clarke / Kubrick/2001 fans "
     ]
    }
   ],
   "source": [
    "tokens=en(txt1)\n",
    "\n",
    "first5=tokens[:50]\n",
    "\n",
    "print(\"The words :\",\"\\n\")\n",
    "for item in first5:\n",
    "    if(not item.is_stop):\n",
    "        print(item,end=\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8fd8fe",
   "metadata": {},
   "source": [
    "## Task4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "305c7a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bryan \t PROPN \t NNP\n",
      "visited \t VERB \t VBD\n",
      "his \t PRON \t PRP$\n",
      "friend \t NOUN \t NN\n",
      "for \t ADP \t IN\n",
      "a \t DET \t DT\n",
      "while \t NOUN \t NN\n",
      "and \t CCONJ \t CC\n",
      "then \t ADV \t RB\n",
      "went \t VERB \t VBD\n",
      "home \t ADV \t RB\n",
      "at \t ADP \t IN\n",
      "10 \t NUM \t CD\n",
      "p.m \t NOUN \t NN\n"
     ]
    }
   ],
   "source": [
    "txt='Bryan visited his friend for a while and then went home at 10 p.m'\n",
    "\n",
    "dic1=en(txt)\n",
    "\n",
    "for word in dic1:\n",
    "    print(word.text,\"\\t\",word.pos_,\"\\t\",word.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8862e1d9",
   "metadata": {},
   "source": [
    "## Task5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5d2b64e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHOOL \t PROPN \t\n",
      "DAY \t PROPN \t\n",
      "Revision \t PROPN \t\n",
      "November \t PROPN \t\n",
      "12 \t NUM \t\n",
      "1997 \t NUM \t\n",
      "ten \t NUM \t\n",
      "Mrs. \t PROPN \t\n",
      "Johnson \t PROPN \t\n",
      "Sharon \t PROPN \t\n",
      "MICHAEL- \t PROPN \t\n",
      "MICHAEL \t PROPN \t\n",
      "Dakota \t PROPN \t\n"
     ]
    }
   ],
   "source": [
    "txt2=open(r\"K:\\Desktop\\NIIT\\Practice\\Course12\\DS3_C2_S3_Sample_Data_Practice.txt\").read()\n",
    "\n",
    "dic2=en(txt2)\n",
    "\n",
    "for word in dic2:\n",
    "    if(word.pos_=='PROPN' or word.tag_=='CD'):\n",
    "        print(word.text,\"\\t\",word.pos_,\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416add44",
   "metadata": {},
   "source": [
    "## Task6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9231c6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words after removal : \n",
      "\n",
      "Note poster Kubrick newsgroup : \n",
      "\n",
      " found bbs ago thought pass \n",
      " Kubrick freaks . \n",
      "\n",
      " 02/23/89 \n",
      " Transcriber note : \n",
      "\n",
      " Clarke / Kubrick/2001 fans , \n",
      "\n",
      " found original paper copy screenplay felt \n",
      " compelled transcribe disk upload bulletin \n",
      " boards enjoyment . \n",
      "\n",
      " final movie deviates screenplay number interesting \n",
      " ways . tried maintain format original document \n",
      " number lines page original . order reduce \n",
      " length file bar \" ------ \" delimit pages \n",
      " lot whitespace original screenplay page . \n",
      " "
     ]
    }
   ],
   "source": [
    "en.Defaults.stop_words -= {'always','never','between','becomes'}\n",
    "\n",
    "first5_1=en(txt1)\n",
    "\n",
    "print(\"The words after removal :\",\"\\n\")\n",
    "for item in first5_1:\n",
    "    if(not item.is_stop):\n",
    "        print(item,end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "32d9d883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words after adding : \n",
      "\n",
      "Note poster Kubrick newsgroup : \n",
      "\n",
      " found bbs ago thought pass \n",
      " Kubrick freaks . \n",
      "\n",
      " 02/23/89 \n",
      " Transcriber note : \n",
      "\n",
      " Clarke / Kubrick/2001 fans , \n",
      "\n",
      " found original paper copy screenplay felt \n",
      " compelled transcribe disk upload bulletin \n",
      " boards enjoyment . \n",
      "\n",
      " final movie deviates screenplay number interesting \n",
      " ways . tried maintain format original document \n",
      " number lines page original . order reduce \n",
      " length file bar \" ------ \" delimit pages \n",
      " lot whitespace original screenplay page . \n",
      " "
     ]
    }
   ],
   "source": [
    "en.Defaults.stop_words |= {'ago','thought','note','after'}\n",
    "\n",
    "first5_2=en(txt1)\n",
    "\n",
    "print(\"The words after adding :\",\"\\n\")\n",
    "for item in first5_2:\n",
    "    if(not item.is_stop):\n",
    "        print(item,end=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "92c17b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming\n",
      "padua high school - day\n",
      "revis novemb 12, 1997\n",
      " \n",
      "\n",
      "Lemmatization\n",
      "PADUA high SCHOOL - DAY \n",
      " Revision November 12 , 1997 \n",
      " \n",
      "Stop word removal\n",
      "PADUA HIGH SCHOOL - DAY \n",
      " Revision November 12 , 1997 \n",
      " \n",
      "Tagging and POS\n",
      "PADUA \t VERB \t\n",
      "HIGH \t ADJ \t\n",
      "SCHOOL \t PROPN \t\n",
      "- \t PUNCT \t\n",
      "DAY \t PROPN \t\n",
      "\n",
      " \t SPACE \t\n",
      "Revision \t PROPN \t\n",
      "November \t PROPN \t\n",
      "12 \t NUM \t\n",
      ", \t PUNCT \t\n",
      "1997 \t NUM \t\n",
      "\n",
      " \t SPACE \t\n"
     ]
    }
   ],
   "source": [
    "random=\"\"\"PADUA HIGH SCHOOL - DAY\n",
    "Revision November 12, 1997\n",
    "\"\"\"\n",
    "\n",
    "print(\"Stemming\")\n",
    "for token in random.split(\" \"):\n",
    "    print(stemmer.stem(token),end=\" \")\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"Lemmatization\")\n",
    "lemmatized=en(random)\n",
    "\n",
    "for item in lemmatized:\n",
    "    print(item.lemma_,end=\" \")\n",
    "print()\n",
    "    \n",
    "print('Stop word removal')\n",
    "for item in lemmatized:\n",
    "    if(not item.is_stop):\n",
    "        print(item,end=\" \")\n",
    "print()\n",
    "\n",
    "print('Tagging and POS')\n",
    "for word in lemmatized:\n",
    "        print(word.text,\"\\t\",word.pos_,\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
